{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1760065353755,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3r4rXoLO9lBH",
    "outputId": "f7169cef-f129-48af-e78e-6393de67b36c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi #this command acts as a task manager between nvidia gpu and the user. Shows GPU related details (system management information)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2q1QDfvec_T"
   },
   "source": [
    "## *Below code is specific to pushing collab notebooks to git, check for text cell below indicating \"Implementation\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYctJq52d_-V"
   },
   "outputs": [],
   "source": [
    "# to clear notebook metadata before pushing it to git. This is so to make notebook readable in git.\n",
    "\n",
    "!pip install nbformat\n",
    "\n",
    "import nbformat\n",
    "\n",
    "# Read notebook\n",
    "with open('AWQ_Implementation.ipynb', 'r') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove problematic metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Clear outputs\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == 'code':\n",
    "        cell.outputs = []\n",
    "        cell.execution_count = None\n",
    "\n",
    "# Write cleaned notebook\n",
    "with open('AWQ_Implementation.ipynb', 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpCXec-teQr5"
   },
   "outputs": [],
   "source": [
    "# git push code\n",
    "\n",
    "!git add AWQ_Implementation.ipynb\n",
    "!git commit -m \"Compression for opt-1.3b complete, 2.5x smaller size\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqd9fFL9eyMO"
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1760065676278,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "aM3-XCZ89pgH",
    "outputId": "98b2f445-d0b5-46be-e5c2-9d38b663530a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1760065703876,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "LgBhrYqL9pis",
    "outputId": "e023978d-5bcb-4f8a-a384-2dd84d068314"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive\n",
    "#switching the directory to my drive to keep all the model related data there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 748,
     "status": "ok",
     "timestamp": 1760065637648,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "f2w7DTYt9plQ",
    "outputId": "4eb9bf16-062a-416c-e795-9817d330c116"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Tanishk-Singh/Transformers.git\n",
    "# cloning the git repo in my directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1760065715883,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ZZs46cnE9pn9",
    "outputId": "7d7357b5-2bab-4d54-d0f4-0aae844019e0"
   },
   "outputs": [],
   "source": [
    "%cd Transformers\n",
    "# switching to transofrmer git repo as directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1760065727205,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "H0Z7v4sx9pqT"
   },
   "outputs": [],
   "source": [
    "!git config user.name \"Tanishk-Singh\"\n",
    "!git config user.email \"tanwartanishk5@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17450,
     "status": "ok",
     "timestamp": 1759917259938,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "CiXp6sG69ptA",
    "outputId": "e8e3fb36-8cf9-4b13-deb2-c54341a05c7e"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7479,
     "status": "ok",
     "timestamp": 1760065810603,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "W1pKuQIw9pvl",
    "outputId": "4d14e52b-9518-4830-def8-f20be21bfef0"
   },
   "outputs": [],
   "source": [
    "!pip install llmcompressor transformers accelerate\n",
    "\n",
    "# original autoawq is depricated and now llcompressor by VLLM maintains it\n",
    "# transformers - hugging face library to download latest llm models\n",
    "# accelerate - hugging face library to load large models efficently on cpu/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4074,
     "status": "ok",
     "timestamp": 1760065825226,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "Q2jCNun79pyQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1760065830902,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "KWfSVfJ39p1I",
    "outputId": "1973c58a-ca86-425e-ebde-2620eb3c1c81"
   },
   "outputs": [],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"AutoAWQ imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1760065891852,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lsKpHKaX9p3w"
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/awq_quantized\n",
    "#creates a folder name awq_quantised in directory models, p is the parent flag, if parent directory doesn't exist\n",
    "# it creates one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1760069772865,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "AkWGB5k9GyP8"
   },
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "b75d8b4846a8404baf86795a49e19da5",
      "0a6f6d936a9a46689f07162cb4548b79",
      "a9aa1b36814745d68c8a7cf903e4bccc",
      "00e601ff482749bd97f280ae0dc92cdb",
      "b57464d46af847bd90dcca3238a43f4d",
      "0801efc050824d4aa775ba96349546d4",
      "77b341e0aaad4fd5a40f2fda928291df",
      "583066cd3407488e99b5ba6b8166795d",
      "292872ba468f48a7b781dc67ca46447d",
      "5e65afc18f9547c096728baa95e11597",
      "370cc86ddf294f888dcd592aa6c7254c",
      "0cdd75b59f3c4dbca427f456afe76f82",
      "43d184fa34f14d24a509daa75b8bdbe5",
      "ec658cfa502a4083b4bad03818fc75e4",
      "aac59669631746d7b34e01b5005cb158",
      "ad53ab0d94764c528aaa9066f5e6a9cc",
      "6055c392a3994b4e80d9b559956c76ba",
      "ef075d51c92041fc9fdd546734f62440",
      "5e1f39402c9347f2bf52babfcb841643",
      "824ea774d90f4cb9bf9b1a8647cc863d",
      "8d3f9abe51ab46c0bd357ee5c1837bc7",
      "43358334b4a74b5eb70cec2bf24d9477",
      "0087775dc1d940b793a8b5bce9a17f4b",
      "f13901ba0fa54065b111cd58dadbe3b6",
      "db2cb96896634e5fbbe50eac84794dc8",
      "7e73284abd6f457b8d25481153a1c119",
      "c985978942ae43d6a777ac2a1bbf3d53",
      "52be3bfc7fd247088cba286c72ba8e67",
      "98cc6522475848e78f81590de7341ca2",
      "eec3813793054997b2fea322f0e7fd66",
      "e32e2c93857a436381be1c5ce3211df9",
      "93a6804e69b64c96a733776abde193d1",
      "e0751a0d310b4f2b9ecadc688b637bf9",
      "ed96d593e8094604badb563a5fac2ed6",
      "0dac47d218314147aa8d4818e0af27ea",
      "2fb4a1cd49fe469089449871c754d532",
      "24cb5ff7d5544f6bbe587a43304295f7",
      "704f9d715776456b99be17ad13a88d26",
      "35738fa560d7468290412dfa1d5096f1",
      "ab6382bbaf384623a06e512b3662844e",
      "38c83520dbbd4d53b139bcd5ab06373d",
      "004cdbbc6858467e9192c802bad6b5f3",
      "0768c16b5a62414f9c8c271d62b236d5",
      "6bf75532890d4556a520d4cc34a13518"
     ]
    },
    "executionInfo": {
     "elapsed": 19492,
     "status": "ok",
     "timestamp": 1760066598803,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "JKnaQGXqSPSS",
    "outputId": "a04415af-8c93-4357-9b1f-ac161a0922e6"
   },
   "outputs": [],
   "source": [
    "model_path = \"facebook/opt-1.3b\" # looks for this model in cache, if not found, loads it from Hugging Face Library to cache\n",
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "#to store our quantised or compresses model\n",
    "\n",
    "print(\"Loading FP16 model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1760068730815,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "UX971RvXKL3r",
    "outputId": "e9f105f9-6b18-4042-9f97-11c69e3b065f"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# we first load tokeniser becasue they convert text to numbers to input into the model\n",
    "\n",
    "fp16_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "# here we iterate every tensor(weight matrix) in the model. Get the no of elements in each and mutiply it by size\n",
    "# then convert total bytest to Gigabytes\n",
    "print(f\"Size of FP16 Parameters loaded: {fp16_size:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453,
     "referenced_widgets": [
      "674a9ef0eb7c4d458a3fcc3d323c923a",
      "bbda92010b9a4747828f2cd7e3edc538",
      "b38e7e391d8747f49e34d4f1cb037fe4",
      "6adf5a1675df423db0e846c9dbd28cc5",
      "64e6449a89cf44a7b8091b01f3ef9626",
      "b43921e8be604779b715cf0d06f66c94",
      "5af45fcdcc3c49b1bfd6e06b5ec18520",
      "b93655eb133f49169b8ca2dce3ff321e",
      "7e8a70c106bc460d9fe96adccc03b8e0",
      "5c671fbcab5346968b9e4ef3a10c9e62",
      "2be9a4a14fc041aeaa63d430d5905d23"
     ]
    },
    "executionInfo": {
     "elapsed": 65478,
     "status": "error",
     "timestamp": 1760070196409,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ktWLuJLBRHq6",
    "outputId": "5ef867a9-e85f-4b7b-f1a0-9d432901031b"
   },
   "outputs": [],
   "source": [
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "\n",
    "print(\"\\nQuantizing with AWQ (W4A16)...\")\n",
    "\n",
    "# AWQ quantization recipe\n",
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        QuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    targets: [\"Linear\"]\n",
    "                    weights:\n",
    "                        num_bits: 4\n",
    "                        type: \"int\"\n",
    "                        symmetric: True\n",
    "                        strategy: \"group\"\n",
    "                        group_size: 128\n",
    "\"\"\"\n",
    "\n",
    "oneshot = oneshot(\n",
    "    model=model,\n",
    "    dataset=\"open_platypus\",\n",
    "    num_calibration_samples=512,\n",
    "    recipe=recipe,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"✅ Quantization complete! Saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1760071149766,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3fuxGMIaaguW",
    "outputId": "7830bd7f-739b-4b70-b68a-4c55088b915e"
   },
   "outputs": [],
   "source": [
    "# Load quantized model\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate size of quanitsed model\n",
    "quant_size = sum(p.numel() * p.element_size() for p in quant_model.parameters()) / 1e9\n",
    "\n",
    "print(f\"Model Size Comparison:\")\n",
    "print(f\"FP16:  {fp16_size:.2f} GB\")\n",
    "print(f\"AWQ:   {quant_size:.2f} GB\")\n",
    "print(f\"Ratio: {fp16_size/quant_size:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8134,
     "status": "ok",
     "timestamp": 1760071778937,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "eMWNYCORKZxW",
    "outputId": "2a21ac02-97c7-47ec-cfac-5744aa7ee481"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhF25fgvKj9A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDaKlmzgqlR00DKj4o3aiG",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
