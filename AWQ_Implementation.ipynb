{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1760065353755,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3r4rXoLO9lBH",
    "outputId": "f7169cef-f129-48af-e78e-6393de67b36c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 10 03:02:34 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi #this command acts as a task manager between nvidia gpu and the user. Shows GPU related details (system management information)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2q1QDfvec_T"
   },
   "source": [
    "## *Below code is specific to pushing collab notebooks to git, check for text cell below indicating \"Implementation\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6126,
     "status": "ok",
     "timestamp": 1760072294402,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "oYctJq52d_-V",
    "outputId": "1acb6a72-69b7-435b-e5d2-3ac32465e0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "Cleaned!\n"
     ]
    }
   ],
   "source": [
    "# to clear notebook metadata before pushing it to git. This is so to make notebook readable in git.\n",
    "\n",
    "!pip install nbformat\n",
    "\n",
    "import nbformat\n",
    "\n",
    "# Read notebook\n",
    "with open('AWQ_Implementation.ipynb', 'r') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove problematic metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Write cleaned notebook\n",
    "with open('AWQ_Implementation.ipynb', 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1760072301861,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lpCXec-teQr5",
    "outputId": "46ac38a0-44ac-4e8c-87f4-278a438ed8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b419c83] Compression for opt-1.3b complete, 2.5x smaller size\n",
      " 1 file changed, 641 insertions(+), 1 deletion(-)\n",
      " rewrite AWQ_Implementation.ipynb (99%)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 4.85 KiB | 827.00 KiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/Tanishk-Singh/Transformers.git\n",
      "   7ae5c73..b419c83  main -> main\n"
     ]
    }
   ],
   "source": [
    "# git push code\n",
    "\n",
    "!git add AWQ_Implementation.ipynb\n",
    "!git commit -m \"Compression for opt-1.3b complete, 2.5x smaller size\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqd9fFL9eyMO"
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1760065676278,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "aM3-XCZ89pgH",
    "outputId": "98b2f445-d0b5-46be-e5c2-9d38b663530a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1760065703876,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "LgBhrYqL9pis",
    "outputId": "e023978d-5bcb-4f8a-a384-2dd84d068314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive\n",
    "#switching the directory to my drive to keep all the model related data there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 748,
     "status": "ok",
     "timestamp": 1760065637648,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "f2w7DTYt9plQ",
    "outputId": "4eb9bf16-062a-416c-e795-9817d330c116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Tanishk-Singh/Transformers.git\n",
    "# cloning the git repo in my directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1760065715883,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ZZs46cnE9pn9",
    "outputId": "7d7357b5-2bab-4d54-d0f4-0aae844019e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Transformers\n"
     ]
    }
   ],
   "source": [
    "%cd Transformers\n",
    "# switching to transofrmer git repo as directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1760065727205,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "H0Z7v4sx9pqT"
   },
   "outputs": [],
   "source": [
    "!git config user.name \"Tanishk-Singh\"\n",
    "!git config user.email \"tanwartanishk5@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17450,
     "status": "ok",
     "timestamp": 1759917259938,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "CiXp6sG69ptA",
    "outputId": "e8e3fb36-8cf9-4b13-deb2-c54341a05c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh index: 100% (5/5), done.\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   AWQ_Implementation.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7479,
     "status": "ok",
     "timestamp": 1760065810603,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "W1pKuQIw9pvl",
    "outputId": "4d14e52b-9518-4830-def8-f20be21bfef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmcompressor\n",
      "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
      "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
      "Requirement already satisfied: datasets<=4.1.1,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
      "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
      "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
      "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (3.12.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru, compressed-tensors, llmcompressor\n",
      "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llmcompressor transformers accelerate\n",
    "\n",
    "# original autoawq is depricated and now llcompressor by VLLM maintains it\n",
    "# transformers - hugging face library to download latest llm models\n",
    "# accelerate - hugging face library to load large models efficently on cpu/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4074,
     "status": "ok",
     "timestamp": 1760065825226,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "Q2jCNun79pyQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1760065830902,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "KWfSVfJ39p1I",
    "outputId": "1973c58a-ca86-425e-ebde-2620eb3c1c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "AutoAWQ imported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"AutoAWQ imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1760065891852,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lsKpHKaX9p3w"
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/awq_quantized\n",
    "#creates a folder name awq_quantised in directory models, p is the parent flag, if parent directory doesn't exist\n",
    "# it creates one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1760069772865,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "AkWGB5k9GyP8"
   },
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "b75d8b4846a8404baf86795a49e19da5",
      "0a6f6d936a9a46689f07162cb4548b79",
      "a9aa1b36814745d68c8a7cf903e4bccc",
      "00e601ff482749bd97f280ae0dc92cdb",
      "b57464d46af847bd90dcca3238a43f4d",
      "0801efc050824d4aa775ba96349546d4",
      "77b341e0aaad4fd5a40f2fda928291df",
      "583066cd3407488e99b5ba6b8166795d",
      "292872ba468f48a7b781dc67ca46447d",
      "5e65afc18f9547c096728baa95e11597",
      "370cc86ddf294f888dcd592aa6c7254c",
      "0cdd75b59f3c4dbca427f456afe76f82",
      "43d184fa34f14d24a509daa75b8bdbe5",
      "ec658cfa502a4083b4bad03818fc75e4",
      "aac59669631746d7b34e01b5005cb158",
      "ad53ab0d94764c528aaa9066f5e6a9cc",
      "6055c392a3994b4e80d9b559956c76ba",
      "ef075d51c92041fc9fdd546734f62440",
      "5e1f39402c9347f2bf52babfcb841643",
      "824ea774d90f4cb9bf9b1a8647cc863d",
      "8d3f9abe51ab46c0bd357ee5c1837bc7",
      "43358334b4a74b5eb70cec2bf24d9477",
      "0087775dc1d940b793a8b5bce9a17f4b",
      "f13901ba0fa54065b111cd58dadbe3b6",
      "db2cb96896634e5fbbe50eac84794dc8",
      "7e73284abd6f457b8d25481153a1c119",
      "c985978942ae43d6a777ac2a1bbf3d53",
      "52be3bfc7fd247088cba286c72ba8e67",
      "98cc6522475848e78f81590de7341ca2",
      "eec3813793054997b2fea322f0e7fd66",
      "e32e2c93857a436381be1c5ce3211df9",
      "93a6804e69b64c96a733776abde193d1",
      "e0751a0d310b4f2b9ecadc688b637bf9",
      "ed96d593e8094604badb563a5fac2ed6",
      "0dac47d218314147aa8d4818e0af27ea",
      "2fb4a1cd49fe469089449871c754d532",
      "24cb5ff7d5544f6bbe587a43304295f7",
      "704f9d715776456b99be17ad13a88d26",
      "35738fa560d7468290412dfa1d5096f1",
      "ab6382bbaf384623a06e512b3662844e",
      "38c83520dbbd4d53b139bcd5ab06373d",
      "004cdbbc6858467e9192c802bad6b5f3",
      "0768c16b5a62414f9c8c271d62b236d5",
      "6bf75532890d4556a520d4cc34a13518"
     ]
    },
    "executionInfo": {
     "elapsed": 19492,
     "status": "ok",
     "timestamp": 1760066598803,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "JKnaQGXqSPSS",
    "outputId": "a04415af-8c93-4357-9b1f-ac161a0922e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FP16 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75d8b4846a8404baf86795a49e19da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdd75b59f3c4dbca427f456afe76f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0087775dc1d940b793a8b5bce9a17f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed96d593e8094604badb563a5fac2ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"facebook/opt-1.3b\" # looks for this model in cache, if not found, loads it from Hugging Face Library to cache\n",
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "#to store our quantised or compresses model\n",
    "\n",
    "print(\"Loading FP16 model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1760068730815,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "UX971RvXKL3r",
    "outputId": "e9f105f9-6b18-4042-9f97-11c69e3b065f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 Parameters loaded: 2.63 GB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# we first load tokeniser becasue they convert text to numbers to input into the model\n",
    "\n",
    "fp16_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "# here we iterate every tensor(weight matrix) in the model. Get the no of elements in each and mutiply it by size\n",
    "# then convert total bytest to Gigabytes\n",
    "print(f\"Size of FP16 Parameters loaded: {fp16_size:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453,
     "referenced_widgets": [
      "674a9ef0eb7c4d458a3fcc3d323c923a",
      "bbda92010b9a4747828f2cd7e3edc538",
      "b38e7e391d8747f49e34d4f1cb037fe4",
      "6adf5a1675df423db0e846c9dbd28cc5",
      "64e6449a89cf44a7b8091b01f3ef9626",
      "b43921e8be604779b715cf0d06f66c94",
      "5af45fcdcc3c49b1bfd6e06b5ec18520",
      "b93655eb133f49169b8ca2dce3ff321e",
      "7e8a70c106bc460d9fe96adccc03b8e0",
      "5c671fbcab5346968b9e4ef3a10c9e62",
      "2be9a4a14fc041aeaa63d430d5905d23"
     ]
    },
    "executionInfo": {
     "elapsed": 65478,
     "status": "error",
     "timestamp": 1760070196409,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ktWLuJLBRHq6",
    "outputId": "5ef867a9-e85f-4b7b-f1a0-9d432901031b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing with AWQ (W4A16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674a9ef0eb7c4d458a3fcc3d323c923a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/24926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-10T04:22:59.502662+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-10-10T04:22:59.559559+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-10-10T04:22:59.560691+0000 | IndependentPipeline | INFO - Inferred `DataFreePipeline` for `QuantizationModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating global scales: 100%|██████████| 144/144 [00:00<00:00, 316551.25it/s]\n",
      "Fusing global scales: 416it [00:00, 288592.53it/s]\n",
      "Calibrating weights: 100%|██████████| 144/144 [00:01<00:00, 89.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-10T04:23:03.875921+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-10T04:23:05.432470+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 144it [00:05, 27.85it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.apply() missing 1 required positional argument: 'fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-857804847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Run quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Quantization complete! Saved to {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.apply() missing 1 required positional argument: 'fn'"
     ]
    }
   ],
   "source": [
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "\n",
    "print(\"\\nQuantizing with AWQ (W4A16)...\")\n",
    "\n",
    "# AWQ quantization recipe\n",
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        QuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    targets: [\"Linear\"]\n",
    "                    weights:\n",
    "                        num_bits: 4\n",
    "                        type: \"int\"\n",
    "                        symmetric: True\n",
    "                        strategy: \"group\"\n",
    "                        group_size: 128\n",
    "\"\"\"\n",
    "\n",
    "oneshot = oneshot(\n",
    "    model=model,\n",
    "    dataset=\"open_platypus\",\n",
    "    num_calibration_samples=512,\n",
    "    recipe=recipe,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"✅ Quantization complete! Saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1760071149766,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3fuxGMIaaguW",
    "outputId": "7830bd7f-739b-4b70-b68a-4c55088b915e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 144it [00:00, 1119.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model Size Comparison:\n",
      "FP16:  2.63 GB\n",
      "AWQ:   1.04 GB\n",
      "Ratio: 2.5x smaller\n"
     ]
    }
   ],
   "source": [
    "# Load quantized model\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate size of quanitsed model\n",
    "quant_size = sum(p.numel() * p.element_size() for p in quant_model.parameters()) / 1e9\n",
    "\n",
    "print(f\"Model Size Comparison:\")\n",
    "print(f\"FP16:  {fp16_size:.2f} GB\")\n",
    "print(f\"AWQ:   {quant_size:.2f} GB\")\n",
    "print(f\"Ratio: {fp16_size/quant_size:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8134,
     "status": "ok",
     "timestamp": 1760071778937,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "eMWNYCORKZxW",
    "outputId": "2a21ac02-97c7-47ec-cfac-5744aa7ee481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh index:  80% (4/5)\r",
      "Refresh index: 100% (5/5)\r",
      "Refresh index: 100% (5/5), done.\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   AWQ_Implementation.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mmodels/\u001b[m\n",
      "\t\u001b[31msparse_logs/\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhF25fgvKj9A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNM5Iy5yVjPKv+U/nW3oaSt",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
