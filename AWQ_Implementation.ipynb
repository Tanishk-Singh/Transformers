{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1759811981577,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3r4rXoLO9lBH",
    "outputId": "91ca391c-cb4a-47b2-fc75-00895eb52291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  7 04:39:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi #this command acts as a task manager between nvidia gpu and the user. Shows GPU related details (system management information)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65304,
     "status": "ok",
     "timestamp": 1759812651981,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "aM3-XCZ89pgH",
    "outputId": "9ffa22b8-0388-47ac-e2dc-fd7e257d61f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1759812673176,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "LgBhrYqL9pis",
    "outputId": "2b544dda-e0a2-434b-d510-c4426900570c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1759812693739,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "f2w7DTYt9plQ",
    "outputId": "823eaa03-0c7c-4b32-a7fa-ad1147972295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Transformers'...\n",
      "remote: Enumerating objects: 11, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 11 (delta 4), reused 6 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (11/11), 5.95 KiB | 1.98 MiB/s, done.\n",
      "Resolving deltas: 100% (4/4), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Tanishk-Singh/Transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1759812715654,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ZZs46cnE9pn9",
    "outputId": "3dd7cef7-fd1d-4263-b04d-d1d6fc255c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Transformers\n"
     ]
    }
   ],
   "source": [
    "%cd Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1759812760353,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "H0Z7v4sx9pqT"
   },
   "outputs": [],
   "source": [
    "!git config user.name \"Tanishk-Singh\"\n",
    "!git config user.email \"tanwartanishk5@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1759812773076,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "CiXp6sG69ptA",
    "outputId": "a4750089-a507-48db-cc21-db2995c7b7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8320,
     "status": "ok",
     "timestamp": 1759813342850,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "W1pKuQIw9pvl",
    "outputId": "7ab90b55-acca-4ad9-dd4a-65477040fcf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq\n",
      "  Downloading autoawq-0.2.9.tar.gz (74 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from autoawq) (2.8.0+cu126)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from autoawq) (3.4.0)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from autoawq) (0.22.1)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from autoawq) (4.15.0)\n",
      "Requirement already satisfied: datasets>=2.20 in /usr/local/lib/python3.12/dist-packages (from autoawq) (4.0.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from autoawq) (0.25.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.12/dist-packages (from autoawq) (0.35.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20->autoawq) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20->autoawq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.20->autoawq) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.26.5->autoawq) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->autoawq) (1.11.1.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (3.12.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->autoawq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->autoawq) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.20->autoawq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n",
      "Building wheels for collected packages: autoawq\n",
      "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for autoawq: filename=autoawq-0.2.9-py3-none-any.whl size=115106 sha256=a0f98f1a8b596a9f88cb9ed94e083dce8e6c6d5e2dce0a572f7f794e009f7be8\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/1a/7b/7314b3a958454e8ce349f600829a3f0a6a05aeebf987be1e16\n",
      "Successfully built autoawq\n",
      "Installing collected packages: autoawq\n",
      "Successfully installed autoawq-0.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq transformers accelerate\n",
    "\n",
    "# autoawq - main quantisation library\n",
    "# transformers - hugging face library to download latest llm models\n",
    "# accelerate - hugging face library to load large models efficently on cpu/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19248,
     "status": "ok",
     "timestamp": 1759813503095,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "Q2jCNun79pyQ",
    "outputId": "56e8d52b-14ef-48c8-964d-e5709b390dbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1759813539706,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "KWfSVfJ39p1I",
    "outputId": "e347a37d-0568-4af6-9a66-cdbe4684ea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "AutoAWQ imported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"AutoAWQ imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1759814174786,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lsKpHKaX9p3w"
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/awq_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818,
     "referenced_widgets": [
      "ea179b96c6ff4f609a8d1fdac8ebf78b",
      "af8be39fcb4547f49b01f39ddf6cf825",
      "c35be8503683493e81e2a1439abe382f",
      "6ccb6f5e9f2f4c1782facc751e4e5776",
      "2631799b312b434999f1f89b5638d82b",
      "ca9cd6b96ceb41ada55c81da55b4455a",
      "44150e1f9fc74f51a078935a1502046b",
      "0f7e6a2fce5341af9e761d8f6e6ed45c",
      "7b50b4c77f964b27a08509781d5cb03c",
      "f4d8688992974d8fba3f2c53db77a02e",
      "db0c7fc798424f54a53d5f2e3fc9b3e1",
      "66bfd80ba8a04ffca3c35fdb7332f7df",
      "4197485c9ed54928876103f4391b4297",
      "2b0e107e756f4c74b7630ca9263489ef",
      "382bafad20cb4a7fbb22d4579f545c59",
      "2a1d0bda44bf49ae9d93833ea3c1873a",
      "e7dcea94d5b541a58e3b63b3b9db0962",
      "051c0b2f422946819080f440503a1cf2",
      "fca2cb06033a4a75928ee810aa36165d",
      "4df82c0f9fb8406fac556e87dc2e6819",
      "01f9df0b964043e0b6b053b4131c4a89",
      "4023e71d9c4a45f58fe264717930da4c",
      "c3846b00e11d46e3b594bbf0f2341dcb",
      "ac387818bc51445aaed9624e2beb3c20",
      "01134cbc2a5e4e6b8d114917dc37caf1",
      "a987bac490434d11af79d39a71470527",
      "612d74844e3042dd9a04af41495bcab0",
      "2792ad048a3443bbaf362a0494c30425",
      "bf58e46785bc4254a2c3f1436c0ecd11",
      "8aef0174cf264a11a1438bba4e4c9906",
      "46343bd648204cea8105faf48fb3e8b7",
      "57b23c9d366549b18ad403877bfb619a",
      "7164d1677ab24e4eaa31424768fb374e",
      "f7afb20387eb49dc9bdc819c9ba8a2fa",
      "31e5074e6a964075aac7f4f288e3ca52",
      "664ac01b5e3640d89d61c7b2005b8f91",
      "79771677d0de4fa39f2472f03a5bd0b5",
      "7cb1f96cbf4c45f4aedd0cd2cabba79e",
      "fa6b29a2662d4fd9a1a235339855d8ff",
      "371c82ecbfbc4553918b2bc87aa37e43",
      "05c4a1a41d71447089a9d83fa96d1851",
      "bb544fb5374f45ec932567678e948b48",
      "1205bc13df5246fb97252a5c8039ff4c",
      "6157bd15a4f64070b0304b06fba33a8b",
      "5728ec26e3dd4c32bd5c6bf3ee05d07d",
      "6edcdeac13dd4bc5b9e0e234caf6c6bc",
      "f3c1a6c8a06949aeb2af3eda1d16c16d",
      "8c0754e4c19c44018a3ca00b08de3484",
      "3cdbbfbff3eb432cac8386208cc96390",
      "eca5a5db075440d3ba64eac688e8ec88",
      "20f78fc0a6634c0fbea45454f41848a4",
      "7df7e19f469e4b81b7963363947eee35",
      "3547bd9450954c37a6254358500074ba",
      "190f2e11788e462eaf8a5ea0e6bcb293",
      "ff0d82824f2c41f79be91814d2e9f495",
      "2db197561c8045c19932a748063ae408",
      "c30d73b5cd6b425e8a4b3ecd04cc8614",
      "19a7e132b41244cd9e4c4f56c2647e14",
      "60b5aed5335840189e92439406b796de",
      "bdb22f55d92447eb9bddaad25ae35373",
      "a7a6e908acee4057bf1b576b7da9c61e",
      "43e99a6cc15949dfbca11e56b94a2559",
      "c6cffad0ab3640bdb4cae5ef717fe12b",
      "12702a3bc040475ead9c4e08b77e77ef",
      "d066b080ebf1412c8e6aa67a1a5d0887",
      "bafc2fae30a444a0904a2d6e10e6a916",
      "1b6dcb301c3c404c970fc7b75468b5ed",
      "1b9d889d2efb429fa060c8091f12ee1f",
      "0ba387ff74744db283ae5b2822536106",
      "357b946fbd824c549662d1911b548a7d",
      "9abcb803fd6f452e9da7ef3e4956cf78",
      "b1759a3512a54408ab99873510df1d66",
      "715e84191ca64ca0a84520f606bac422",
      "a16d7a337ba74e619991421423f759f1",
      "ba147fd0008c446ba2848bf861d931e0",
      "f15b598651c248e09889fc00cd8ff70a",
      "8dbbd69e74784c75a160d52f95ef4973",
      "e8b7a2d5550d46ff9fad8534be20267a",
      "7c396c9da85043d1856451d5c27d40c4",
      "1fe2befd729948d0891729a16988fb2e",
      "e02d464ac0ac4b0abf8d2be58b9f7423",
      "3d63570bfd19438c9362738bd1798190",
      "eafaed3d0ffc4a71947282c0d6e24e42",
      "ba371620aa7f4f79a7e9ba62768e72c5",
      "edf92e5c188d4362a34c14e8cc28a3f5",
      "486ad42e6f534e3db61edf79370b0cc4",
      "6ebef67ff78645d58643d234cc64609c",
      "b405b54966ca447b89d206b36d9385e9",
      "e385f4d23b874204a6f728bbb8dad607",
      "d75f178b7a9649a6aeb50021d53924f5",
      "ec2aa25da25c444ea829e5c4dd1b795f",
      "72b640b0129a4d199cac22d4d0ba12e8",
      "92903f7de62d4139a9bd1d93427d0345",
      "77f4f0252372496db65dbe46a6be4ea2",
      "c7b3e89660d04e2c9e7ab7d50f858e03",
      "27b5c95985c147cc9261e2ed0cb1381a",
      "4c4ae5beae55458495e60242ad3eb3a4",
      "2895857227d144faa0db159546e24d22",
      "9319360bea584108a62a6e50e9f7fc7d",
      "94427fb890fe40d8b54ddb46bc2ecae3",
      "c96010fb4e0d445db7bda38ca2e12d43",
      "68983bc41d5843508f00c104ddc0ff02",
      "972eb81b77cb4ffab6be5c4bad8963cd",
      "3c7095fc0ba04a79b474144854142a10",
      "5f02f3ab1fe24602bd4dd29827632f09",
      "59afd20cf1a5417396a842f93dfe999b",
      "428c38c8da4045589262511a55e707da",
      "b132b31ca8ac49daa7272912406cf2e4",
      "1dbea00903ab447c9e62339cd3296b39",
      "f9a8a3f717a0478ea2bc03ccacf42308"
     ]
    },
    "executionInfo": {
     "elapsed": 554,
     "status": "error",
     "timestamp": 1759814762127,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "AkWGB5k9GyP8",
    "outputId": "edd42467-3f70-41b8-a202-2b3eb8f35b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OPT-1.3B model (this will take 2-3 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea179b96c6ff4f609a8d1fdac8ebf78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bfd80ba8a04ffca3c35fdb7332f7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3846b00e11d46e3b594bbf0f2341dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7afb20387eb49dc9bdc819c9ba8a2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5728ec26e3dd4c32bd5c6bf3ee05d07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db197561c8045c19932a748063ae408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6dcb301c3c404c970fc7b75468b5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b7a2d5550d46ff9fad8534be20267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e385f4d23b874204a6f728bbb8dad607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94427fb890fe40d8b54ddb46bc2ecae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named model.safetensors found in directory /root/.cache/huggingface/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-819887944.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Download with force_download to ensure clean download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = AutoAWQForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/awq/models/auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, model_path, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_init_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n\u001b[0;32m---> 83\u001b[0;31m         return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_pretrained(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/awq/models/base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, model_path, model_type, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# If not quantized, must load with AutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         model = target_cls.from_pretrained(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mmodel_weights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5028\u001b[0m             )\n\u001b[1;32m   5029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5030\u001b[0;31m         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[0m\u001b[1;32m   5031\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5032\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 )\n\u001b[1;32m   1096\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0muse_safetensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                 raise OSError(\n\u001b[0m\u001b[1;32m   1098\u001b[0m                     \u001b[0;34mf\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)} found in directory\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0;34mf\" {pretrained_model_name_or_path}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named model.safetensors found in directory /root/.cache/huggingface/hub/models--facebook--opt-1.3b/snapshots/3f5c25d0bc631cb57ac65913f76e22c2dfb61d62."
     ]
    }
   ],
   "source": [
    "# Clear the corrupted cache\n",
    "!rm -rf /root/.cache/huggingface/hub/models--facebook--opt-1.3b\n",
    "\n",
    "# Force re-download with explicit parameters\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_path = \"facebook/opt-1.3b\"\n",
    "\n",
    "print(\"Downloading OPT-1.3B model (this will take 2-3 minutes)...\")\n",
    "\n",
    "# Download with force_download to ensure clean download\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    force_download=False,  # Will use cache if valid, re-download if corrupt\n",
    "    resume_download=True   # Resume if interrupted\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"📊 Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911,
     "referenced_widgets": [
      "43f3efff693e40c0a0ce98b4ed52d61e",
      "c9166ce7dd6647b787020c789d16b695",
      "23c16728129e473fa259f1905ac38e64",
      "25565053da8b4141b46bf9b95a0bb4da",
      "2342bdfa3d0447b5bb064859cc32454a",
      "5475f072c3034d8984631f713b909c9f",
      "7c8facf7577c41e89ddd3f2c4903ce04",
      "a5e8d531df464615b405bdbc5d3ce8a8",
      "1b83ccdff3404aa791884a78f9062122",
      "ed4aa95ffc854bbf9eaa7037ed10db89",
      "ef8869a768824d658c022a154a081655",
      "5bc67bbfb1bc498ea23efda01bbd9395",
      "e75528272b764537be9a0c7429b39c57",
      "11771972516e442eb49bfa11b310808c",
      "20bb315075c2478298d612f6e7da0635",
      "cd7a8b260ff14d55b6e27ae0df7c525c",
      "c2e0ba158f0e4bc2bd1ec0734730c41d",
      "052745159eb74f9ea4de5a4b75cd8344",
      "79eb834c346844f5bf128c8784caa3ac",
      "06c817057e254ce2a2b3e7bc49a582cf",
      "d845fbbe0de740808e3f9f0234b1c99d",
      "8282a2883d7e474cb8ffef66f21a088a",
      "71b552057762473a9f18f82e93388905",
      "f84267b94bb447f9ba080c398b01fff3",
      "ec79516dc24e4b70afe3a3ec9cd2eda4",
      "30c31e3cb8564c879e17a9651bc0e15c",
      "0c070cdbf79b44f884adc299a206d802",
      "263acf10b1624180980f699527590636",
      "223afd29e3fc4960af502ef1374a26a0",
      "a90b2682ccc445efadb78fac8bc405a8",
      "6b416a558888441998c1c83712553d37",
      "631240a40ee943049da2883cf9e47636",
      "5c9ae62b0bc64be8ac2a181dbaec81ef",
      "b9a34d8849b744fa9b602230bc973d03",
      "d1733237be00474ca2f2f6a05ed27958",
      "bc4b406df81d4e10a7deb33689b0e818",
      "f23e5cde202d4e91a670f33b332f26dc",
      "482114e9e8864dffa4745b687b137c10",
      "45c7835352834b08b30b653ec72003ee",
      "2955f0dcce9e4e3cb93b8b1901bfbb8c",
      "2d36c672087540e3a99030184457257a",
      "3c4aa5a333bc47bf815dd3432ab2ad8a",
      "8df0d3ead8ee4cdab939e8b0317b8c66",
      "48ec55c0e0ec45e2b46cf824bda4caaf",
      "4b5aadb97a5c4df581b53c5935b94099",
      "2f3a3c5badce43fb816ddb0d4bff26d3",
      "13d4ecc6e32a4807b22b35ed5bdf270b",
      "101ce0be5c90433abde8b3a4feeb6574",
      "046435029473437d8d9ce16bb5bb779e",
      "2965542b9a6c4b0382a47833bf585189",
      "c93dd3efc4f94612830688e1d25bed77",
      "0f80d120e93946b69d42ca3f7ee81007",
      "5b7d8c7c86aa4717b0b1ba2b26090ee7",
      "da4c3de5ce2744aca0d7f8505fca7ee8",
      "a3d6d44bc4e543ea93de71271271285b",
      "1312f89ca899485d82057acc587b77f7",
      "fa379bf9cdfb4cc3bfe285fb04eacfe2",
      "64ff5922b415401eacef4df1d4977962",
      "2d26d21a002f4cd8920d09b66f3a2f86",
      "69333ae7f99f4a0fb2a0d532d09b4dfa",
      "971a8153f3824f45a90566c1fdc8b73a",
      "3dcb693ddb5a4a6ca8d1c60a2976ae25",
      "605dce8476b445028b6a05c0eeb00b9b",
      "e3b4013c8e354b559b05796690062491",
      "156b4f570bce4a1293592db5619bfa00",
      "70b92394c2a0434e83d951ec2b208853",
      "4f00cf388bea4924ba91a764f424909f",
      "015db7af990740618bfc67e62709211e",
      "ce4b2e6b106a47a19e7139ba807da3dd",
      "9dad3365a42d43ddb376decb517eae0d",
      "bad4e7e94d694dcfa9b4e8c639ac2c24",
      "3bbfc3e2d37f43069f59aa205d4d6dad",
      "c14c890e34f14cfcbe2ccdaf8fb004a6",
      "f91e1ae2e08240adbe7941e5a82e6f1e",
      "8282e9270a7b42e8bc8f909878f52620",
      "1f51a38412d847e5bc96ff0b295bc2d2",
      "cb85eeafe89a44ac8ff26d5675575ea3",
      "b18c89d8ad184ba5967d9cc36c755d79",
      "379d43f2e0cf4ba29a0fc8330f9561da",
      "527d359ccbf3411d9d3c6650543b26cf",
      "abefc51f84314a4e8af82548f5361257",
      "e1bbccee55264e82bc14cb922d963aaf",
      "cb64f236ce7f4426afca217193516aa4",
      "4ac9097b13f3498e818fe0c83c849d72",
      "0f660114fe624e19bcdac8244120e743",
      "9025b11942ab49199162f554fafd8e08",
      "159534efee5b489498f65e75acb9cd0d",
      "3569553ecdc940cf9e49f01c068f6dd6",
      "6cca09b1f9e64ce5a53b585943296da0",
      "5e2dfe4073ba4f35a1be5738ce84f7a7",
      "a4f3503032054fc39a7f29be0cc1ff16",
      "4908d0cc18ea41ecbf360821d9dcf606",
      "2985e7338f21416eafeb4a060f136775",
      "a9fcb05ecec0419fbaf8dba92ba4fbab",
      "f67f994ca7c3435abc1b3064d4559a7d",
      "8226f07c5685496e9895a89ac9f5baf4",
      "2bd86024ab614b7cb57963aebedae85d",
      "55f871f7e4584308920fefee2b9a9993",
      "5b40692e270f4a9cb90f433fb91d2111",
      "f5d9defcc83443fab02dbd191b65deb4",
      "2f9e13286a1449f28a9bf12735fc6701",
      "80ed8f40264f4932b7abba2b9844c0be",
      "15c5287958524397a6f2555e76338e2e",
      "b3ea926066a14e1b91830936bc89b101",
      "7cfb2b0827e74c65954f429db70de25c",
      "ec8c9049589048849ee5e42d9fcc940e",
      "4cd165f9e6c24f97944552a2298cfccf",
      "88fee044d0e648c08c88c1bb4db8b929",
      "1f5a0e4dcf4b44c4aab8f16a226db5a5",
      "24df20802d42459f98eaea54ca711164",
      "82aeffc0fac34c9c9100386aab0e6f96",
      "dd406364dce842149ee9170838e1e071",
      "dd8e318b63b74861a1ccaf029911e621",
      "3426e3f7dd4d47d4a8cd9094e70e3938",
      "7fbddf3b5b784dd0b6c48fb6120dff65",
      "27c001a1ed8f483b8c73426da71269da",
      "078fe31e62254e948e289d73984c0f47",
      "2e013bf654b14e208d3cb22e54fd9dc2",
      "9f9a9147c02e4c5eac8a529487c1b372",
      "ea9155b5793b4404ab6083c5cc5f5b00",
      "58a2e7ea8c264e7899bc2300e758b55e",
      "529b249b723042c6b26ab34af7fe4857",
      "0b65f277a3684950b68775ad64f3a182",
      "61a3dec1d74341bcb04f4d47b52220f0",
      "61c5b9c19680412fba3c138a6fc5c034",
      "467e42032e7c482dad213d92fa54e4ad",
      "a41769915e574ad09e4ae32e4657e274",
      "c859a3e09e0b4832929bbfd57b4b9a5f",
      "b494caac42304c3ea8f62062b03791ab",
      "5d8ec452cc704ed89e299b3d01207792",
      "279cfea2d9ec4d5dabf7c30c9a7e7a39",
      "174e2a91c9444844b35b69e4f299c5d9",
      "94f65cbcb6ec4d689106000e0be184db",
      "90a9bab8aa2843fd93e92ae4f28d34fe",
      "774ba3e282304820872088013fa26152",
      "b27879efa4bb449990b59180adebdbf2",
      "aca287f578cf479cabed4bc24b7ec46b",
      "5482db85fcdb4c7b93ad0d005cb4c53e",
      "651f43a18c5240c1a27e84205f3fd9cc",
      "939c6b4d00804006818e911c7f24bc8d",
      "c3109238adfd45dfb07a3c799e1445a5",
      "443c9409f7bf4a79abebde4c06df1f3e",
      "10b52a6c55ea49ad826f24a37eadd4d8",
      "44fa68e7f45a46de89c3be06f9059487",
      "f5a1ab0a793c4ca9b2ace4e6bcc1ad2c",
      "d7d2b72e379e442db856f327c8a32a58",
      "24f2a0fcaca34078a7dbd5efa3a56baa",
      "71d8c01b2e6b4592aac6eb0d74682461",
      "3343cb20f2f64139853bb59b286b4fc8",
      "bb2ab5d1983b45d893e30402b4577c30",
      "9374b6911e7246c9864fc5b3d51b1e11",
      "7921f0d122dc48f8828f6cd68479cb29",
      "b177fe7258be4bc9a26ceab792502b48",
      "189e90bdac4b4ab8b1b525576c946df9"
     ]
    },
    "executionInfo": {
     "elapsed": 21494,
     "status": "error",
     "timestamp": 1759814975389,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "yQQQbzQeI0Go",
    "outputId": "c4dd16f3-bb7c-48c4-8168-e15ea98c86f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing fresh download of OPT-1.3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f3efff693e40c0a0ce98b4ed52d61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc67bbfb1bc498ea23efda01bbd9395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b552057762473a9f18f82e93388905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a34d8849b744fa9b602230bc973d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5aadb97a5c4df581b53c5935b94099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flax_model.msgpack:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1312f89ca899485d82057acc587b77f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f00cf388bea4924ba91a764f424909f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18c89d8ad184ba5967d9cc36c755d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cca09b1f9e64ce5a53b585943296da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9defcc83443fab02dbd191b65deb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82aeffc0fac34c9c9100386aab0e6f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529b249b723042c6b26ab34af7fe4857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f65cbcb6ec4d689106000e0be184db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fa68e7f45a46de89c3be06f9059487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'use_safetensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3762790365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Now load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m model = AutoAWQForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/awq/models/auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, model_path, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_init_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n\u001b[0;32m---> 83\u001b[0;31m         return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_pretrained(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/awq/models/base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, model_path, model_type, torch_dtype, trust_remote_code, safetensors, device_map, download_kwargs, low_cpu_mem_usage, use_cache, **model_init_kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# If not quantized, must load with AutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         model = target_cls.from_pretrained(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mmodel_weights_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'use_safetensors'"
     ]
    }
   ],
   "source": [
    "# Nuclear option - completely remove cache and force fresh download\n",
    "!rm -rf /root/.cache/huggingface/hub/models--facebook--opt-1.3b\n",
    "\n",
    "# Also clear any partial downloads\n",
    "!rm -rf /root/.cache/huggingface/hub/.locks\n",
    "\n",
    "# Now download with force_download=True\n",
    "from huggingface_hub import snapshot_download\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"facebook/opt-1.3b\"\n",
    "\n",
    "print(\"Forcing fresh download of OPT-1.3B...\")\n",
    "\n",
    "# Download model files first\n",
    "snapshot_download(\n",
    "    repo_id=model_path,\n",
    "    cache_dir=\"/root/.cache/huggingface/hub\",\n",
    "    force_download=True,\n",
    "    resume_download=False\n",
    ")\n",
    "\n",
    "# Now load the model\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    use_safetensors=False # Added this line\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(f\"✅ Model loaded!\")\n",
    "print(f\"📊 Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1759815811814,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "UX971RvXKL3r",
    "outputId": "e7df682c-1cc9-45e6-ecbb-6c1e8b23fee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 259\n",
      "-rw------- 1 root root 239874 Oct  7 05:40 AWQ_Implementation.ipynb\n",
      "drwx------ 8 root root   4096 Oct  7 05:39 .git\n",
      "-rw------- 1 root root   1096 Oct  7 04:51 LICENSE\n",
      "drwx------ 3 root root   4096 Oct  7 05:16 models\n",
      "-rw------- 1 root root   4253 Oct  7 04:51 quantization_min_max_percentile_range.ipynb\n",
      "-rw------- 1 root root     82 Oct  7 04:51 README.md\n",
      "-rw------- 1 root root   9731 Oct  7 04:51 understanding_quantization.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1759815818676,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "eMWNYCORKZxW",
    "outputId": "eee1c528-0bff-49bd-bdb9-ec08de29cc86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mAWQ_Implementation.ipynb\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhF25fgvKj9A"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1759816592196,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "WpsGWcllKgAI",
    "outputId": "31a4ee67-6796-4d8d-f7ca-7cf18cda2dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 3a859df] Phase 1: AutoAWQ installation attempts on Colab, not successful due to Autoawq being depricated\n",
      " Date: Tue Oct 7 05:43:53 2025 +0000\n",
      " 1 file changed, 1 insertion(+)\n",
      " create mode 100644 AWQ_Implementation.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit --amend --no-edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5723,
     "status": "ok",
     "timestamp": 1759816851296,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "FseiEszYQf8S",
    "outputId": "023970da-af4c-495c-edec-cb4a7f078640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "✅ Notebook metadata cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Clean the notebook metadata\n",
    "!pip install nbformat\n",
    "\n",
    "# Run this Python code to fix the notebook\n",
    "import nbformat\n",
    "\n",
    "# Read the notebook\n",
    "with open('AWQ_Implementation.ipynb', 'r') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove problematic widget metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Write back the cleaned notebook\n",
    "with open('AWQ_Implementation.ipynb', 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Notebook metadata cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1759816899341,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "8XUUgqtSQjIC",
    "outputId": "27a90fa1-8b5e-47c9-c5f8-5f1dbd9cac31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 8c6960a] Fixed notebook metadata for GitHub rendering\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 1.43 KiB | 182.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/Tanishk-Singh/Transformers.git\n",
      "   3a859df..8c6960a  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add AWQ_Implementation.ipynb\n",
    "!git commit -m \"Fixed notebook metadata for GitHub rendering\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1759816608181,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "yu0lzZ_oPkUD",
    "outputId": "96a5a013-9fd6-4a27-9894-9432c7044def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 4, done.\n",
      "Counting objects:  25% (1/4)\r",
      "Counting objects:  50% (2/4)\r",
      "Counting objects:  75% (3/4)\r",
      "Counting objects: 100% (4/4)\r",
      "Counting objects: 100% (4/4), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects:  33% (1/3)\r",
      "Compressing objects:  66% (2/3)\r",
      "Compressing objects: 100% (3/3)\r",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects:  33% (1/3)\r",
      "Writing objects:  66% (2/3)\r",
      "Writing objects: 100% (3/3)\r",
      "Writing objects: 100% (3/3), 24.13 KiB | 2.19 MiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas:   0% (0/1)\u001b[K\r",
      "remote: Resolving deltas: 100% (1/1)\u001b[K\r",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/Tanishk-Singh/Transformers.git\n",
      "   42f942b..3a859df  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/polhBmQQZ8S8GaDZj9Ru",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
