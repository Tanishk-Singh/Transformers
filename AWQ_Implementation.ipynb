{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1760075281341,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3r4rXoLO9lBH",
    "outputId": "ba94ada4-d970-4b46-9109-52607b23e494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 10 05:48:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi #this command acts as a task manager between nvidia gpu and the user. Shows GPU related details (system management information)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2q1QDfvec_T"
   },
   "source": [
    "## ***Below code is specific to pushing collab notebooks to git, check for text cell below indicating \"Implementation\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5037,
     "status": "ok",
     "timestamp": 1760072432855,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "oYctJq52d_-V",
    "outputId": "ceec4aa3-e22f-4003-fde9-6731452a8fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.8.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n",
      "Cleaned!\n"
     ]
    }
   ],
   "source": [
    "# to clear notebook metadata before pushing it to git. This is so to make notebook readable in git.\n",
    "\n",
    "!pip install nbformat\n",
    "\n",
    "import nbformat\n",
    "\n",
    "# Read notebook\n",
    "with open('AWQ_Implementation.ipynb', 'r') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Remove problematic metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Write cleaned notebook\n",
    "with open('AWQ_Implementation.ipynb', 'w') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"Cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1760072437756,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lpCXec-teQr5",
    "outputId": "b2a5a018-1951-4578-c5db-7bb4ff20b17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 4f5b142] Compression for opt-1.3b complete, 2.5x smaller size\n",
      " 1 file changed, 490 insertions(+), 43 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 9.10 KiB | 1.01 MiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/Tanishk-Singh/Transformers.git\n",
      "   b419c83..4f5b142  main -> main\n"
     ]
    }
   ],
   "source": [
    "# git push code\n",
    "\n",
    "!git add AWQ_Implementation.ipynb\n",
    "!git commit -m \"Compression for opt-1.3b complete, 2.5x smaller size\"\n",
    "!git push\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqd9fFL9eyMO"
   },
   "source": [
    "# **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17403,
     "status": "ok",
     "timestamp": 1760075312410,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "aM3-XCZ89pgH",
    "outputId": "e135748b-d571-4d56-ef00-4e637cd11ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1760075313753,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "LgBhrYqL9pis",
    "outputId": "0ecd485d-ee1b-4672-92e6-9e80256dd29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive\n",
    "#switching the directory to my drive to keep all the model related data there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1760075318636,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "f2w7DTYt9plQ",
    "outputId": "53b16b61-1f00-4094-c421-a5f0d9bc66be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Tanishk-Singh/Transformers.git\n",
    "# cloning the git repo in my directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760075320633,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ZZs46cnE9pn9",
    "outputId": "70998533-1697-4449-8f01-634e65dd9f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Transformers\n"
     ]
    }
   ],
   "source": [
    "%cd Transformers\n",
    "# switching to transofrmer git repo as directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1408,
     "status": "ok",
     "timestamp": 1760075324302,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "H0Z7v4sx9pqT"
   },
   "outputs": [],
   "source": [
    "!git config user.name \"Tanishk-Singh\"\n",
    "!git config user.email \"tanwartanishk5@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7246,
     "status": "ok",
     "timestamp": 1760075339778,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "W1pKuQIw9pvl",
    "outputId": "10ac787c-f691-4a39-ff51-3f413cefae0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmcompressor\n",
      "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
      "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
      "Requirement already satisfied: datasets<=4.1.1,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
      "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
      "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
      "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.1.1,>=4.0.0->llmcompressor) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (3.12.15)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.1.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru, compressed-tensors, llmcompressor\n",
      "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llmcompressor transformers accelerate\n",
    "\n",
    "# original autoawq is depricated and now llcompressor by vLLM maintains it\n",
    "# transformers - hugging face library to download latest llm models\n",
    "# accelerate - hugging face library to load large models efficently on cpu/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3766,
     "status": "ok",
     "timestamp": 1760075378894,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "Q2jCNun79pyQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1760075380150,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "KWfSVfJ39p1I",
    "outputId": "f5bb8e61-2210-447d-cc30-a21d75488d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "AutoAWQ imported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"AutoAWQ imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1760075383652,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "lsKpHKaX9p3w"
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/awq_quantized\n",
    "#creates a folder name awq_quantised in directory models, p is the parent flag, if parent directory doesn't exist\n",
    "# it creates one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 20738,
     "status": "ok",
     "timestamp": 1760075407274,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "AkWGB5k9GyP8"
   },
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "4a65c1da5ea4489a9ec55fabfacffaa9",
      "f538082347a14938a142f3aff893b545",
      "496739af01db4f95aff00f2677f11932",
      "87c2af1ba91f4ce8863975759cd4504e",
      "2eb5bcf805444a669a6e5cf874fff6c6",
      "1127ac2113384b60a5eac3a676e4f0e2",
      "a5af2e64ccd74854b289ff84f250f894",
      "c2e0fe87e2d34582ac4545a0e86a8c9c",
      "bde32d415372465eb1645bc95514137c",
      "de2f32610c8d4791b56685f2159d112a",
      "008ca15fe2cc42be8c6cded4d0fbacd7",
      "4fa63ce708d34d2fa6dcaeb8520629fd",
      "7fc95d64548149b99d6a96e9d6782e63",
      "4352bf9685024c8ebeab18bff588c64f",
      "6bf4c4c36361430eaef993af39de6bff",
      "38543fcc00ea4d3587da223b815ed31c",
      "4e578297a3db4abc954792d8cbd44175",
      "129689beb8f7447eb38e1b1ea274c4e3",
      "5873e4f3681b4f2087d5256bc00b3a27",
      "f8d74ee1ba3941148e76a349780d9345",
      "3af8a885df8c450499dc995756b63cc2",
      "08809c772bf2448cb7c4bc0df6e2eda9",
      "41cd9e0470d242048d5b2cd94d554a67",
      "0bc106e2bad14e73a814a757ed687b63",
      "6b2e8d2d8a8f47db843f037da7fcbfc5",
      "147b077f86754bf0998bccd0a46fbd7d",
      "9998e5bafc27493b912609372e0ceac8",
      "885e595dab0a42e09c104f5d4f7c2761",
      "ae490a531549490e82420e5b2ac3b27b",
      "a2941239ac474598bf88ba210a1d9eb8",
      "41724dc3695349cf963f91c02e7f436b",
      "bed1cb0ccc614404a57b7b9b9f07a420",
      "554b6c16e1df44fbb3d130404b5522ff",
      "bb23861a24d74cfd8bccbfdfad47492a",
      "22c2d57887c8489e93c30d15ced1b6a8",
      "a9beacd1930d46ee95b7e1fbe489d074",
      "c237b9c533be4ab494a92e17633b98e0",
      "3037fe617ba04853a12027f98d5ac9aa",
      "bf898bf5edbf49c29ea6d1679bea4284",
      "e02ec89e2fd54c2998a7645f41230fdd",
      "d2b270ca0d324684a19e8261ce80288a",
      "68f2ac460e3c4b97bd60718b66fc8f53",
      "bfd9b6ad7c2941f3b44c63768deaa03a",
      "143a2805c0b44df0931385ad8200fd00"
     ]
    },
    "executionInfo": {
     "elapsed": 18792,
     "status": "ok",
     "timestamp": 1760075603888,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "JKnaQGXqSPSS",
    "outputId": "25fed465-031a-49ae-e8c7-3b677412f4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FP16 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a65c1da5ea4489a9ec55fabfacffaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa63ce708d34d2fa6dcaeb8520629fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cd9e0470d242048d5b2cd94d554a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb23861a24d74cfd8bccbfdfad47492a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"facebook/opt-1.3b\" # looks for this model in cache, if not found, loads it from Hugging Face Library to cache\n",
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "#to store our quantised or compresses model\n",
    "\n",
    "print(\"Loading FP16 model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "7db3b284a46a4a2fa493606cef32a58a",
      "0fdfba498276451d99871294683ac4c8",
      "75241b6d4a9d44a3b6328619b4d5d0e6",
      "6417ebac94224a989954bf4aef4b2f24",
      "d6cb26dde8fc494cb6abd3a27661e790",
      "5f691722e65848d089dfbd723d2bdecc",
      "0cbdfb56e8454db3be355df6a8916a16",
      "dcca17e8bfc7406ca65af2fdd22a7288",
      "168d03e744014a0fbc390242709d9281",
      "5e7a7b5270fc45fc9ca8e3c8d044e193",
      "12c5f110f3db4839bfbff5ebb93a407b",
      "aeb2e9e723cb4f32b9ba1febfefae711",
      "bad0a4a8015940a4af365e9f75ffb29f",
      "7cfd12124aca4ccc9f30e0df8b6ac225",
      "eb4de2b7615746d79714d28d01fb9845",
      "e2e482eaab5842a48ff556f178781030",
      "08fad91c599f43a3bf54a2ef7dc7a06a",
      "99c5255a103a421c8e83ccbb4ed19f38",
      "fe62e3b4a8214ba48e0c84186189e9c6",
      "1e8c3ce4dcf540fd81339958fe9bf991",
      "9c0b7e9981f740f881fba4f3e2a890a3",
      "50c940eaaffe443ca8fec063570160ac",
      "9681728ee23244e4b2b0f5e31b337b56",
      "4e8239e5a04e4a11b194b23e0e58edef",
      "ac3f8d009f314d2caedf68f46da4408d",
      "99e470012f3648729611241aae0c5e0d",
      "01422e7dee8b47eebfd50c950428758a",
      "60ba10ce37a44f169092355255112d05",
      "6c93b5edadc44f54b730a78fb4645741",
      "6aa3ae37d4814b0bbf6cd7fd8fe0e07a",
      "dde4305ade424d0fa227f25ed5db1de4",
      "30debf10f1f743ed8d3b4e30e346f35e",
      "b7e8afbd473745cc9df88218a4046194",
      "afc35bcfdf274eda863bae6b28196101",
      "650720577e0b49738663232287816507",
      "994e0dcc18924af8beaa9428fbd1e418",
      "4cf98bce5b684cffa960b90d37b4c370",
      "191bc05a78014658adee179ca8c10164",
      "5ff79f0d0e9541a281a6292a68a52512",
      "6227a744ddb34375a55909bae7239416",
      "2db745c576174d8ba188e864e1b51c0d",
      "56d72eabdbcc4fc7bac882f520052524",
      "88e2983ab3744aec8704bdd930c5c21d",
      "fa524ecdb63643c9b2ae1dd7cec96aca"
     ]
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1760075625244,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "UX971RvXKL3r",
    "outputId": "ee608ab7-a76f-4aa8-f0a8-f2e6b84d99a8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db3b284a46a4a2fa493606cef32a58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb2e9e723cb4f32b9ba1febfefae711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9681728ee23244e4b2b0f5e31b337b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc35bcfdf274eda863bae6b28196101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 Parameters loaded: 2.63 GB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# we first load tokeniser becasue they convert text to numbers to input into the model\n",
    "\n",
    "fp16_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e9\n",
    "# here we iterate every tensor(weight matrix) in the model. Get the no of elements in each and mutiply it by size\n",
    "# then convert total bytest to Gigabytes\n",
    "print(f\"Size of FP16 Parameters loaded: {fp16_size:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "b74ed140bb034700bc8e98bf11aeeee9",
      "11afceb5d35446c6a24ccdb044fa96b8",
      "9d2f4c14ab4147499722763c68f1c13a",
      "48053602c43c483b896a9c4cc42961a8",
      "55fb1685939c4f29be6b67c0b8e544e9",
      "40310d23a43a48e3bea56a3e1fed7ff8",
      "62a6543766ed4b4c86123b6cc4658b69",
      "2051288d209d4ac0834eee972719dff6",
      "0005840b786348a280dcbc0a0233b4e6",
      "3fae302b097740be8520dd26da609cd3",
      "56ecce168a7b407e8e5c247f1d1a079d",
      "c20710dc94b043b0bd6ed7af87dc1abc",
      "a78fb0db9842420396a0aabbbc58f1c2",
      "4167762a49424eaa8353715ac5210c31",
      "b1f8b65075cd4a0bbc15d7bce0440afc",
      "0c740b857e9244cb805f1a61c437c3d4",
      "ed29d18c7cef42cf9399cff6907bc13f",
      "e78fe0d2c5814bce9a2ad3bf31e39987",
      "c2b40e1846ea4f3f9f4b52380c651e4b",
      "3cc7922c226e4ec688047b3405073b22",
      "bcf4ecf1919547a287ec2f561622bda2",
      "18044f27dd324196a212e793143a7d65",
      "c2c4e531ff424d8e84af3171072c2ee7",
      "29fa74db9d6b4b5dbc0e8ab4b872308a",
      "d4d3a7d01d334f288e9ea3c6129641ef",
      "791b87f09b1746f0a515abc3cc6cf950",
      "b2d8bbfc833441bcb1f0a83f8a91ad0a",
      "85d79285de434e3ca406a7351c73b904",
      "a47867521b034ccdbb691eb4424fc6db",
      "9ccf1451879441dd9faf19451311a652",
      "220ce9e0f3a548e48c6ebcf3e35f5968",
      "af5d3ecb214748a08b604a958ee5e18b",
      "6c9c6f87e13744adb75f83834ab1667b",
      "520ae046835c4ece8e020575dc590f5c",
      "bff876200c274ec3be88da07fcc43177",
      "2a883920f74c4bda894300c4831517ae",
      "9c16d88166294492bedfd89a80433081",
      "0fe6d2d546704906a95616737528a752",
      "6c9dcc327c8d405c8021534bda2503b8",
      "7cce5a77da90456d8b56462bd992c5ed",
      "d215928a704e4255ba16c67562d7394c",
      "af4ef04223cb4a84bc282218bb42d0bb",
      "bfd7a30c5faa4c22a76e7bd16a7a9e0e",
      "18d0e4ad1805430bbb83f99c7098836f",
      "9096b46b667b4d02a628664561985c23",
      "75c4903af6904eac92403bcc1858a62f",
      "d870c5716b084c3b866e7fc02a87398a",
      "189c232220144f7e8c12656aaa55d2cb",
      "1973998951e6401390e892a5f8ca4d7a",
      "72f5926b2ffc4016a33e7d782fd3a28d",
      "b12cb70d22134be982d4bdeddc51cb7a",
      "15c1c6b057ab4cdebe8fa2011907a9df",
      "773e732b57ed4ad984747c9733cc81b6",
      "1b46c6d832204710ad5076fc4a69649c",
      "732f31ecf5ed4dcab3abcbd50a2bd240"
     ]
    },
    "executionInfo": {
     "elapsed": 86031,
     "status": "ok",
     "timestamp": 1760075726661,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "ktWLuJLBRHq6",
    "outputId": "0567b46a-4e4e-450a-845c-f614bb4d37c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantizing with AWQ (W4A16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74ed140bb034700bc8e98bf11aeeee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20710dc94b043b0bd6ed7af87dc1abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-4fe2df04669d16(…):   0%|          | 0.00/15.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c4e531ff424d8e84af3171072c2ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520ae046835c4ece8e020575dc590f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/24926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9096b46b667b4d02a628664561985c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/24926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-10T05:54:51.281055+0000 | reset | INFO - Compression lifecycle reset\n",
      "2025-10-10T05:54:51.290248+0000 | _create_default_logger | INFO - Logging all LLM Compressor modifier-level logs to sparse_logs/10-10-2025_05.54.51.log\n",
      "2025-10-10T05:54:51.394906+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2025-10-10T05:54:51.395990+0000 | IndependentPipeline | INFO - Inferred `DataFreePipeline` for `QuantizationModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating global scales: 100%|██████████| 144/144 [00:00<00:00, 363186.88it/s]\n",
      "Fusing global scales: 416it [00:00, 311855.31it/s]\n",
      "Calibrating weights: 100%|██████████| 144/144 [00:01<00:00, 84.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-10T05:54:55.713580+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
      "2025-10-10T05:54:57.287292+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 144it [00:05, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization complete and Saved to /content/drive/MyDrive/Transformers/models/opt-1.3b-awq\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\"\n",
    "\n",
    "print(\"\\nQuantizing with AWQ (W4A16)...\")\n",
    "\n",
    "# AWQ quantization recipe\n",
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        QuantizationModifier:\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    targets: [\"Linear\"]\n",
    "                    weights:\n",
    "                        num_bits: 4\n",
    "                        type: \"int\"\n",
    "                        symmetric: True\n",
    "                        strategy: \"group\"\n",
    "                        group_size: 128\n",
    "\"\"\"\n",
    "\n",
    "# above we ignore lm_head as its the final layer and we don't want to quanitse the weights in it to maintain accuracy.\n",
    "# num bits is set to 4 for INT 4 quntisation\n",
    "# we chose symmetric quantisation (quantise weights symetrically in the given range)as weights after scaling in AWQ method are centred around zero and symmetric quanitsation gives better inference overall\n",
    "# group size is AWQ technique mentioned in the paper, go for group instead of individual tensors(weight matrices)\n",
    "\n",
    "oneshot = oneshot(\n",
    "    model=model,\n",
    "    dataset=\"open_platypus\", # this is sample dataset for AWQ, to identify important weights and set scaling factor\n",
    "    num_calibration_samples=512, # similar batch size as mentioned in AWQ paper\n",
    "    recipe=recipe,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"Quantization complete and Saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1526,
     "status": "ok",
     "timestamp": 1760075762504,
     "user": {
      "displayName": "Tanishk Singh",
      "userId": "12859907109530834890"
     },
     "user_tz": 420
    },
    "id": "3fuxGMIaaguW",
    "outputId": "560a3dbb-4fcc-44e4-99ff-f3a64b8617cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 144it [00:00, 1143.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size Comparison:\n",
      "FP16:  2.63 GB\n",
      "AWQ:   1.04 GB\n",
      "Ratio: 2.5x smaller\n"
     ]
    }
   ],
   "source": [
    "# Load quantized model\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/content/drive/MyDrive/Transformers/models/opt-1.3b-awq\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate size of quanitsed model\n",
    "quant_size = sum(p.numel() * p.element_size() for p in quant_model.parameters()) / 1e9\n",
    "\n",
    "print(f\"Model Size Comparison:\")\n",
    "print(f\"FP16:  {fp16_size:.2f} GB\")\n",
    "print(f\"AWQ:   {quant_size:.2f} GB\")\n",
    "print(f\"Ratio: {fp16_size/quant_size:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhF25fgvKj9A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP0ZWJN+TYatXmUMSH58l6f",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
